{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5STeGqw63l__"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pronobis/libspn-keras/blob/master/examples/notebooks/Benchmark%20With%20Einsum%20Networks.ipynb)\n",
    "# Bench-marking LibSPN-Keras against EinsumNetworks\n",
    "Recently, [Peharz et al.](https://arxiv.org/abs/2004.06231) proposed _Einsum Networks_, which are essentially SPNs with a specific implementation style. Einsum Networks implement a range of tricks that can be used to easily achieve tensor-based building of SPNs as well as EM learning through _autograd_. \n",
    "\n",
    "Fortunately, `libspn-keras` already had the same tricks implemented _ever since it was made available_ (before the Einsum paper was published), and so it was straightforward to set up a side-by-side comparison of `libspn-keras` \n",
    "against the open-source implementation of Einsum Networks.\n",
    "\n",
    "## Installing Frameworks\n",
    "`libspn-keras` is available on PyPI and so we'll simply use `pip` to install it. For `Einsum` we'll need to clone its repo and add it to our `PATH`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "6bGer1RP3mAF",
    "outputId": "9a736743-d95c-424d-8623-6d3eee97f03d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'EinsumNetworks' already exists and is not an empty directory.\n",
      "Processing ./libspn_keras-0.4.0-py3-none-any.whl\n",
      "Installing collected packages: libspn-keras\n",
      "Successfully installed libspn-keras-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cambridge-mlg/EinsumNetworks & pip install libspn_keras --ignore-installed --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGLXv70ol-ZX"
   },
   "source": [
    "\n",
    "\n",
    "## Bench-marking Architecture\n",
    "This notebook takes an architecture that was made available in the original Einsum Network repo. We increase the depth of the network and the number of sums and input distributions to get a better idea of how the `libspn-keras` and `Einsum` implementations scale.\n",
    "\n",
    "The architecture is an SPN that recursively splits variables in two balanced groups starting at the root node. The groups are assigned randomly. Splitting is done up until a `depth` (in this case 5). After that, the remaining groups are combined through 'factorized input distributions'. This means that the binary leaf indicators are each weighted and the multiplied with the remaining nodes in a group. This is repeated `num_repetitions` times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p0yeY0Xe3mAB"
   },
   "outputs": [],
   "source": [
    "depth = 5\n",
    "num_repetitions = 32\n",
    "num_input_distributions = 32\n",
    "num_sums = 32\n",
    "\n",
    "max_num_epochs = 3\n",
    "batch_size = 100\n",
    "online_em_frequency = 1\n",
    "online_em_stepsize = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPp--fdHiqbE"
   },
   "source": [
    "## Running Einsum\n",
    "The public Einsum implementation uses a Pytorch backend. We'll run it for 10 epochs on a binary dataset and report\n",
    "- The number of parameters per layer\n",
    "- The log likelihood on validation data once every epoch\n",
    "- The log likelihood on test data at the very end\n",
    "- The total time spent training and computing the log-likelihood for the validation at each epoch\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "For the most interesting comparison, make sure the GPU is actually being used. You can select a GPU in Colab using the `Runtime` dropdown which should show a `Change runtime type` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "8eIlR4Bc3mAL",
    "outputId": "c6d5977b-00b7-48f5-a2e2-c86c8598bed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cuda\n",
      "EinsumNetwork(\n",
      "  (einet_layers): ModuleList(\n",
      "    (0): FactorizedLeafLayer(\n",
      "      (ef_array): CategoricalArray()\n",
      "    )\n",
      "    (1): EinsumLayer()\n",
      "    (2): EinsumLayer()\n",
      "    (3): EinsumLayer()\n",
      "    (4): EinsumLayer()\n",
      "    (5): EinsumLayer()\n",
      "    (6): EinsumMixingLayer()\n",
      "  )\n",
      ")\n",
      "Number of input dist parameters\n",
      "227328\n",
      "Number of remaining sum parameters\n",
      "[16777216, 8388608, 4194304, 2097152, 32768, 32]\n",
      "Total trainable parameters\n",
      "31717408\n",
      "[0]   valid LL -76.9047509765625\n",
      "[1]   valid LL -37.21986155790441\n",
      "[2]   valid LL -36.9792524988511\n",
      "Total time: 43.69 seconds\n",
      "-37.363624970523205\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"EinsumNetworks/src\")\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from EinsumNetwork import Graph, EinsumNetwork\n",
    "import datasets\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Running on \", device)\n",
    "\n",
    "dataset = 'accidents'\n",
    "\n",
    "train_x_orig, test_x_orig, valid_x_orig = datasets.load_debd(dataset, dtype='float32')\n",
    "\n",
    "train_x = train_x_orig\n",
    "test_x = test_x_orig\n",
    "valid_x = valid_x_orig\n",
    "\n",
    "# to torch\n",
    "train_x = torch.from_numpy(train_x).to(torch.device(device))\n",
    "valid_x = torch.from_numpy(valid_x).to(torch.device(device))\n",
    "test_x = torch.from_numpy(test_x).to(torch.device(device))\n",
    "\n",
    "train_N, num_dims = train_x.shape\n",
    "valid_N = valid_x.shape[0]\n",
    "test_N = test_x.shape[0]\n",
    "\n",
    "graph = Graph.random_binary_trees(num_var=train_x.shape[1], depth=depth, num_repetitions=num_repetitions)\n",
    "\n",
    "args = EinsumNetwork.Args(\n",
    "    num_classes=1,\n",
    "    num_input_distributions=num_input_distributions,\n",
    "    exponential_family=EinsumNetwork.CategoricalArray,\n",
    "    exponential_family_args={'K': 2},\n",
    "    num_sums=num_sums,\n",
    "    num_var=train_x.shape[1],\n",
    "    online_em_frequency=1,\n",
    "    online_em_stepsize=0.05)\n",
    "\n",
    "einet = EinsumNetwork.EinsumNetwork(graph, args)\n",
    "einet.initialize()\n",
    "einet.to(device)\n",
    "print(einet)\n",
    "\n",
    "print(\"Number of input dist parameters\")\n",
    "input_layer_param_count = np.prod(einet.einet_layers[0].ef_array.params.shape)\n",
    "print(input_layer_param_count)\n",
    "\n",
    "print(\"Number of remaining sum parameters\")\n",
    "remaining_param_counts = [np.prod(layer.params.shape) for layer in einet.einet_layers if hasattr(layer,'params')]\n",
    "print(remaining_param_counts)\n",
    "\n",
    "print(\"Total trainable parameters\")\n",
    "print(sum([input_layer_param_count] + remaining_param_counts))\n",
    "\n",
    "start = time.time()\n",
    "for epoch_count in range(max_num_epochs):\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        # This bit differs from original implementation \n",
    "        # (which did not disable grad, in other words this is to make this\n",
    "        # part of the script run slightly faster)\n",
    "        valid_ll = EinsumNetwork.eval_loglikelihood_batched(einet, valid_x)\n",
    "\n",
    "    # Also here we only report the validation LLH during our training epochs\n",
    "    # which makes it similar to how one would do it using Keras (or libspn-keras)\n",
    "    print(\"[{}]   valid LL {}\".format(epoch_count, valid_ll / valid_N))\n",
    "    # train\n",
    "    idx_batches = torch.randperm(train_N).split(batch_size)\n",
    "    for batch_count, idx in enumerate(idx_batches):\n",
    "        batch_x = train_x[idx, :]\n",
    "        outputs = einet.forward(batch_x)\n",
    "\n",
    "        ll_sample = EinsumNetwork.log_likelihoods(outputs)\n",
    "        log_likelihood = ll_sample.sum()\n",
    "\n",
    "        objective = log_likelihood\n",
    "        objective.backward()\n",
    "\n",
    "        einet.em_process_batch()\n",
    "\n",
    "    einet.em_update()\n",
    "\n",
    "print(f\"Total time: {time.time() - start:.2f} seconds\")\n",
    "with torch.no_grad():\n",
    "    test_ll = EinsumNetwork.eval_loglikelihood_batched(einet, test_x)\n",
    "print(test_ll / test_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsX8X_qqjogG"
   },
   "source": [
    "## Quick Analysis\n",
    "We see the following interesting things:\n",
    "- A test LLH of around -37.36\n",
    "- A total train time of 43.69 seconds\n",
    "- Total number of trainable parameters 31,717,408 \n",
    "\n",
    "## LibSPN-Keras Implementation\n",
    "Instead of hiding all the complexity behind a `Graph.random_binary_trees`, `libspn-keras` offers a layer-based approach that offers the ability to control all network parameters flexibly at every layer. In that sense, we are in no sense forced to use the same number of sums for every layer, nor a fixed number of 2 children per product. We can easily vary all of these parameters. Nevertheless, for the sake of comparison we'll reimplement the above architecture using `libspn-keras`.\n",
    "\n",
    "Let's see how they compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "5HuP4SJT3mAO",
    "outputId": "57f50dbf-3397-4839-e4cb-0ad997ce520f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_sum_product_network_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flat_to_regions_3 (FlatToReg (None, 111, 32, 1)        0         \n",
      "_________________________________________________________________\n",
      "indicator_leaf_3 (IndicatorL (None, 111, 32, 2)        0         \n",
      "_________________________________________________________________\n",
      "dense_sum_19 (DenseSum)      (None, 111, 32, 32)       227328    \n",
      "_________________________________________________________________\n",
      "permute_and_pad_scopes_rando (None, 128, 32, 32)       4096      \n",
      "_________________________________________________________________\n",
      "reduce_product_3 (ReduceProd (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_product_16 (DenseProdu (None, 16, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "dense_sum_20 (DenseSum)      (None, 16, 32, 32)        16777216  \n",
      "_________________________________________________________________\n",
      "dense_product_17 (DenseProdu (None, 8, 32, 1024)       0         \n",
      "_________________________________________________________________\n",
      "dense_sum_21 (DenseSum)      (None, 8, 32, 32)         8388608   \n",
      "_________________________________________________________________\n",
      "dense_product_18 (DenseProdu (None, 4, 32, 1024)       0         \n",
      "_________________________________________________________________\n",
      "dense_sum_22 (DenseSum)      (None, 4, 32, 32)         4194304   \n",
      "_________________________________________________________________\n",
      "dense_product_19 (DenseProdu (None, 2, 32, 1024)       0         \n",
      "_________________________________________________________________\n",
      "dense_sum_23 (DenseSum)      (None, 2, 32, 32)         2097152   \n",
      "_________________________________________________________________\n",
      "dense_product_20 (DenseProdu (None, 1, 32, 1024)       0         \n",
      "_________________________________________________________________\n",
      "dense_sum_24 (DenseSum)      (None, 1, 32, 1)          32768     \n",
      "_________________________________________________________________\n",
      "root_sum_3 (RootSum)         (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 31,721,504\n",
      "Trainable params: 31,717,408\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "128/128 [==============================] - 7s 58ms/step - loss: 41.6101 - llh: -41.6101 - val_loss: 37.1047 - val_llh: -37.1047\n",
      "Epoch 2/3\n",
      "128/128 [==============================] - 7s 55ms/step - loss: 37.1014 - llh: -37.1014 - val_loss: 36.8422 - val_llh: -36.8422\n",
      "Epoch 3/3\n",
      "128/128 [==============================] - 7s 55ms/step - loss: 36.9422 - llh: -36.9422 - val_loss: 36.7074 - val_llh: -36.7074\n",
      "Total time: 23.43 seconds\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.1249 - llh: -37.1249\n"
     ]
    }
   ],
   "source": [
    "import libspn_keras as spnk\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_ratspn(num_vars, depth):\n",
    "    # Set global settings\n",
    "    spnk.set_default_sum_op(spnk.SumOpEMBackprop())\n",
    "    spnk.set_default_accumulator_initializer(tf.keras.initializers.RandomUniform())\n",
    "    \n",
    "    # Define stack\n",
    "    sum_product_stack = [\n",
    "        # Create repetitions, nodes and scope axes\n",
    "        spnk.layers.FlatToRegions(num_decomps=num_repetitions, input_shape=[num_vars], dtype=tf.int32),\n",
    "        # Add binary variables\n",
    "        spnk.layers.IndicatorLeaf(num_components=2), \n",
    "        # Add dense sum layer (effectively computing the input distributions)\n",
    "        spnk.layers.DenseSum(num_sums=num_input_distributions),\n",
    "        # Randomly permute scopes and pad if necessary\n",
    "        spnk.layers.PermuteAndPadScopesRandom(),\n",
    "        # Products through reduction correspond to Einsum's notion of \n",
    "        # 'factorized' distributions\n",
    "        spnk.layers.ReduceProduct(num_factors=int(np.ceil(num_vars / 2 ** depth))),\n",
    "    ]\n",
    "    for i in range(depth):\n",
    "        # The alternating stacks of remaining product and sum layers\n",
    "        sum_product_stack += [\n",
    "            spnk.layers.DenseProduct(num_factors=2),\n",
    "            spnk.layers.DenseSum(num_sums=num_sums if i < depth - 1 else 1),\n",
    "        ]\n",
    "    sum_product_stack.append(\n",
    "        spnk.layers.RootSum(return_weighted_child_logits=False)\n",
    "    )\n",
    "    return spnk.models.SequentialSumProductNetwork(sum_product_stack)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x_orig,)).shuffle(10_000).batch(batch_size)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_x_orig,)).batch(len(valid_x_orig))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x_orig,)).batch(len(test_x_orig))\n",
    "\n",
    "spn = build_ratspn(train_x_orig.shape[1], depth=depth)\n",
    "\n",
    "spn.compile(\n",
    "    loss=spnk.losses.NegativeLogLikelihood(),\n",
    "    metrics=[spnk.metrics.LogLikelihood()],\n",
    "    optimizer=spnk.optimizers.OnlineExpectationMaximization(learning_rate=0.05)\n",
    ")\n",
    "\n",
    "spn.summary()\n",
    "\n",
    "start = time.time()\n",
    "spn.fit(train_ds, validation_data=valid_ds, epochs=max_num_epochs)\n",
    "print(f\"Total time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "evaluation = spn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiM-eHeCicUt"
   },
   "source": [
    "## Analysis\n",
    "We now see the following\n",
    "- A test LLH of around -37.12 (not significantly different which makes sense)\n",
    "- A total train time of **23.43** seconds\n",
    "- Total number of trainable parameters 31,717,408 (same as above just as a sanity check)\n",
    "\n",
    "In other words, `libspn-keras` is not just more flexible, but also almost twice as fast here!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Benchmark With Einsums.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
