{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative learning for discrete MNIST data using randomly structured SPNs\n",
    "This notebook shows how to build a randomly structured SPN and train it using a TensorFlow optimizer on binarized MNIST data.\n",
    "\n",
    "### Setting up the imports and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libspn.examples.utils.dataiterator import DataIterator\n",
    "\n",
    "# Load\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def binarize(x):\n",
    "    return np.where(np.greater(x / 255., 0.25), 1.0, 0.0)\n",
    "\n",
    "def flatten(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return binarize(flatten(x)).astype(int), np.expand_dims(y, axis=1)\n",
    "\n",
    "# Preprocess\n",
    "train_x, train_y = preprocess(train_x, train_y)\n",
    "test_x, test_y = preprocess(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of variable subsets that a product joins\n",
    "num_subsets = 2\n",
    "# Number of sums per scope\n",
    "num_mixtures = 4\n",
    "# Number of decompositions per product layer\n",
    "num_decomps = 1\n",
    "# Generate balanced subsets -> balanced tree\n",
    "balanced = True\n",
    "# Number of variables\n",
    "num_vars = train_x.shape[1]\n",
    "# Input distribution. Raw corresponds to first layer being product that \n",
    "# takes raw indicators\n",
    "input_dist = spn.DenseSPNGenerator.InputDist.RAW\n",
    "# Number of different values at leaf (binary here, so 2)\n",
    "num_leaf_values = 2\n",
    "# Initial value for path count accumulators\n",
    "initial_accum_value = 0.1\n",
    "# Inference type (can also be spn.InferenceType.MPE) where \n",
    "# sum nodes are turned into max nodes\n",
    "inference_type = spn.InferenceType.MARGINAL\n",
    "# Other params\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the SPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPN depth: 21\n",
      "Number of products layers: 100\n",
      "Number of sums layers: 90\n"
     ]
    }
   ],
   "source": [
    "# Leaf nodes\n",
    "leaf_indicators = spn.IVs(num_vals=num_leaf_values, num_vars=num_vars)\n",
    "\n",
    "# Generates densely connected random SPNs\n",
    "dense_generator = spn.DenseSPNGenerator(\n",
    "    num_subsets=num_subsets, num_mixtures=num_mixtures, num_decomps=num_decomps, \n",
    "    balanced=balanced, input_dist=input_dist)\n",
    "\n",
    "# Generate a dense SPN for each class\n",
    "class_roots = [dense_generator.generate(leaf_indicators) for _ in range(num_classes)]\n",
    "\n",
    "# Connect sub-SPNs to a root\n",
    "root = spn.Sum(*class_roots, name=\"RootSum\")\n",
    "\n",
    "# Add an IVs node to the root as a latent class variable\n",
    "class_indicators = root.generate_ivs()\n",
    "\n",
    "# Generate the weights for the SPN rooted at `root`\n",
    "spn.generate_weights(root)\n",
    "\n",
    "print(\"SPN depth: {}\".format(root.get_depth()))\n",
    "print(\"Number of products layers: {}\".format(root.get_num_nodes(node_type=spn.ProductsLayer)))\n",
    "print(\"Number of sums layers: {}\".format(root.get_num_nodes(node_type=spn.SumsLayer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op for initializing all weights\n",
    "weight_init_op = spn.initialize_weights(root)\n",
    "# Op for getting the log probability of the root\n",
    "root_log_prob = root.get_log_value(inference_type=inference_type)\n",
    "\n",
    "# Set up ops for discriminative GD learning\n",
    "gd_learning = spn.GDLearning(\n",
    "    root=root, learning_task_type=spn.LearningTaskType.SUPERVISED,\n",
    "    learning_method=spn.LearningMethodType.DISCRIMINATIVE)\n",
    "optimizer = tf.train.AdamOptimizer(beta1=0.95, beta2=0.95)\n",
    "\n",
    "# Use post_gradients_ops = True to also normalize weights (and clip Gaussian variance)\n",
    "gd_update_op = gd_learning.learn(optimizer=optimizer, post_gradient_ops=True)\n",
    "\n",
    "# Compute predictions and matches\n",
    "mpe_state = spn.MPEState()\n",
    "root_marginalized = spn.Sum(*root.values, weights=root.weights)\n",
    "marginalized_ivs = root_marginalized.generate_ivs(\n",
    "    feed=-tf.ones_like(class_indicators.feed)) \n",
    "predictions, = mpe_state.get_state(root_marginalized, marginalized_ivs)\n",
    "match_op = tf.equal(tf.to_int64(predictions), tf.to_int64(class_indicators.feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display TF Graph\n",
    "Only works with Chrome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spn.display_tf_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some convenient iterators\n",
    "train_iterator = DataIterator([train_x, train_y], batch_size=batch_size)\n",
    "test_iterator = DataIterator([test_x, test_y], batch_size=batch_size)\n",
    "\n",
    "def fd(x, y):\n",
    "    return {leaf_indicators: x, class_indicators: y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize things\n",
    "    sess.run([tf.global_variables_initializer(), weight_init_op])\n",
    "    \n",
    "    # Do one run for test likelihoods\n",
    "    matches = []\n",
    "    for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "        batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "        matches.extend(batch_matches.ravel())\n",
    "        test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "    mean_test_accuracy = np.mean(matches)\n",
    "    \n",
    "    print(\"Before training test accuracy = {:.2f}\".format(mean_test_accuracy))                              \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train\n",
    "        matches = []\n",
    "        for batch_x, batch_y in train_iterator.iter_epoch(\"Training\"):\n",
    "            batch_matches, _ = sess.run(\n",
    "                [match_op, gd_update_op], fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            train_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_train_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Test\n",
    "        matches = []\n",
    "        for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "            batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_test_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Report\n",
    "        print(\"Epoch {}, train accuracy = {:.2f}, test accuracy = {:.2f}\".format(\n",
    "            epoch, mean_train_accuracy, mean_test_accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
